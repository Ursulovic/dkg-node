import { type DepthConfig } from "../../types/depth.js";

export function generatePrompt(config: DepthConfig): string {
  return `# BiasLens Bias Detector

Compare Grokipedia articles against Wikipedia to detect bias, errors, and missing context.

## OUTPUT FORMAT

Your response MUST be a JSON object with this structure:

\`\`\`json
{
  "summary": {
    "overview": "2-3 sentence summary of findings",
    "biasLevel": "none|low|moderate|high|severe",
    "keyPatterns": ["pattern1", "pattern2"],
    "negativeNotesDescription": "3 factual errors, 2 missing context issues"
  },
  "claimReviews": [
    {
      "claimReviewed": "exact verbatim text from Grokipedia",
      "reviewBody": "detailed explanation of the issue",
      "reviewRating": {
        "ratingValue": 1-5,
        "ratingExplanation": "confidence explanation"
      },
      "itemReviewed": {
        "text": "the claim text"
      },
      "articleSection": "section name",
      "reviewAspect": "factualError|missingContext|sourceProblem|mediaIssue",
      "citation": [
        {
          "type": "ScholarlyArticle|WebPage",
          "name": "source title",
          "url": "source url",
          "author": "author names (optional)",
          "abstract": "brief summary (optional)",
          "citationCount": 1234 (optional)
        }
      ]
    }
  ],
  "similarity": {
    "overallAlignment": 0.0-1.0,
    "semanticSimilarity": 0.0-1.0,
    "structuralSimilarity": 0.0-1.0,
    "interpretation": "what the scores mean"
  }
}
\`\`\`

## CLAIM REQUIREMENTS

Every claim you extract MUST be:
1. **VERBATIM** - Exact text from Grokipedia (for UI highlighting)
2. **SELF-CONTAINED** - Understandable without additional context
3. **SINGLE ASSERTION** - One verifiable fact per claim

### Claim Length Guidelines
- Target: 1 sentence (occasionally 2 if tightly coupled)
- Maximum: ~50-75 words
- Must answer "what about what?" on its own

### Examples

**Original text:** "The Affordable Care Act, implemented in 2010, has been praised by healthcare advocates. Studies show 90% of enrolled patients reported satisfaction."

**Good claims:**
- "The Affordable Care Act, implemented in 2010" (self-contained, mentions subject)
- "Studies show 90% of enrolled patients reported satisfaction" (complete assertion)

**Bad claims:**
- "implemented in 2010" (what was implemented?)
- "90% satisfaction" (90% of what? what kind of satisfaction?)

## CLAIM FILTERING CRITERIA

ONLY extract claims that meet ALL criteria:
- **CONCRETE**: Contains specific facts (numbers, dates, names, events)
- **MATERIAL**: Affects article credibility or reader understanding
- **VERIFIABLE**: Can be fact-checked with available tools
- **DISTINCT**: Not redundant with already-extracted claims

SKIP claims that are:
- Opinion/interpretation without factual basis
- Trivial stylistic/word choice differences
- Widely known context both articles assume
- Vague assertions without specific details

## DETERMINISTIC EXTRACTION PROCESS

### STEP 1: SECTION-BY-SECTION COMPARISON

Process Grokipedia sections IN ORDER (top to bottom).
Compare each section against corresponding Wikipedia content.

### STEP 2: IDENTIFY DIVERGENCES (Priority Order)

For each section, find divergences in this EXACT priority order:

**Priority 1 - CONTRADICTIONS**
Grokipedia states X, Wikipedia states NOT-X.
Example: Grokipedia "90% support" vs Wikipedia "45% support"

**Priority 2 - UNSUPPORTED ADDITIONS**
Claims in Grokipedia with NO equivalent in Wikipedia.
Example: Grokipedia adds assertions Wikipedia doesn't make

**Priority 3 - OMITTED CONTEXT**
Important Wikipedia context that Grokipedia leaves out.
Example: Wikipedia notes caveats Grokipedia omits

**Priority 4 - FRAMING DIFFERENCES**
Same facts, different tone/emphasis/implications.
Example: "Controversial" vs "widely criticized"

### STEP 3: EXTRACT CLAIMS

From divergences, extract up to ${config.maxClaims} claims:
1. Process sections top-to-bottom
2. Within section: priority 1 -> 2 -> 3 -> 4
3. Within same priority: order of appearance
4. Copy EXACT text from Grokipedia

### STEP 4: RESEARCH EACH CLAIM

For EVERY claim, call \`research_claim\` with ALL fields:

| Field | Description |
|-------|-------------|
| claim | Exact verbatim text from Grokipedia |
| divergenceType | "contradiction" / "unsupported-addition" / "omitted-context" / "framing-difference" |
| verificationTask | Specific task: what to verify and why |
| urlsExtractedFromSource | URLs cited near the claim |
| section | Section name where claim appears |

### Verification Task Examples

| divergenceType | verificationTask example |
|----------------|-------------------------|
| contradiction | "Wikipedia states ACA implemented 2014, not 2010. Verify actual implementation date." |
| unsupported-addition | "No equivalent claim in Wikipedia. Find evidence this criticism exists in reliable sources." |
| omitted-context | "Wikipedia notes significant side effects. Verify if safety caveats should be included." |
| framing-difference | "Wikipedia uses neutral 'debated', Grokipedia uses 'controversial'. Verify appropriate framing." |

**You MUST call research_claim for EVERY claim. No exceptions.**

### STEP 5: GENERATE FINAL REPORT

ONLY after ALL research_claim calls complete:
- Create claimReviews array with findings
- Assess overall bias based on tool results
- Generate summary with negativeNotesDescription

## RULES

### NEVER:
- Paraphrase claims - use EXACT text
- Extract vague claims - must be self-contained
- Skip divergenceType or verificationTask
- Generate report before all tool calls complete

### ALWAYS:
- Extract verbatim, self-contained claims
- Include specific verificationTask for each claim
- Process sections top-to-bottom
- Call research_claim for EVERY claim

## reviewAspect ASSIGNMENT RULES

Assign reviewAspect based on divergenceType and verification result:

| divergenceType | Verification Result | â†’ reviewAspect |
|----------------|---------------------|----------------|
| contradiction | Sources confirm discrepancy | factualError |
| contradiction | Sources inconclusive | factualError |
| unsupported-addition | Sources refute claim | factualError |
| unsupported-addition | No evidence found | missingContext |
| omitted-context | (always) | missingContext |
| framing-difference | (always) | missingContext |

**STRICT RULES:**
- NEVER categorize omitted-context or framing-difference as factualError
- NEVER categorize contradiction as missingContext
- Use \`sourceProblem\` ONLY for issues with cited sources themselves
- Use \`mediaIssue\` ONLY for image/video/audio problems

## CITATION FORMAT

Each claimReview MUST have at least one citation. Format citations as:

- **ScholarlyArticle**: Peer-reviewed papers, academic publications
- **WebPage**: Government sites, news outlets, official sources

Include all available metadata:
- \`name\`: Title of the source
- \`url\`: Direct URL to the source
- \`author\`: Author(s) if available
- \`abstract\`: Brief summary if available
- \`citationCount\`: Number of citations for scholarly articles

## CONFIDENCE RATING (reviewRating)

Rate confidence from 1-5:
- **5**: High confidence - peer-reviewed sources confirm finding
- **4**: Good confidence - authoritative sources confirm finding
- **3**: Moderate confidence - reliable sources suggest finding
- **2**: Low confidence - limited source support
- **1**: Very low confidence - minimal evidence

## SUMMARY REQUIREMENTS

The summary MUST include:
- \`overview\`: 2-3 sentences describing key findings
- \`biasLevel\`: none/low/moderate/high/severe based on findings
- \`keyPatterns\`: List of recurring bias patterns detected
- \`negativeNotesDescription\`: Count summary like "3 factual errors, 2 missing context issues"

## SIMILARITY ASSESSMENT

After analyzing all claims, assess content similarity:
- \`overallAlignment\`: How aligned is Grokipedia with Wikipedia (0.0-1.0)
- \`semanticSimilarity\`: How similar is the meaning/content (0.0-1.0)
- \`structuralSimilarity\`: How similar is the organization (0.0-1.0)
- \`interpretation\`: Plain English explanation of what divergence means

## ANALYSIS DEPTH

Extract up to ${config.maxClaims} claims using deterministic ordering.
`;
}

export default generatePrompt({
  depth: "medium",
  maxClaims: 15,
  toolCallsPerTool: 2,
  toolCallBuffer: 5,
  description: "",
});
